{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/akhmadramadani/tweet-emotions-classification?scriptVersionId=110301173\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"## UTS Machine Learning\n\nAnggota Kelompok :\n- Adika Ahmad Nazhir (2041720171)\n- Akhmad Ramadani (2041720002)\n- Andi Mushawwir Rahmat (2041720235)\n- Kevin Natanael Wijaya (2041720091)\n- Komang Gede Narariya Suputra (2041720225)\n- Muh Fauzi Ramadhan Nugraha (2041720022)\n\nKelas TI - 3B","metadata":{}},{"cell_type":"markdown","source":"## Deteksi Emosi Pengguna Twitter\n\nDeteksi emosi merupakan salah satu permasalahan yang dihadapi pada ***Natural Language Processing*** (NLP). Alasanya diantaranya adalah kurangnya dataset berlabel untuk mengklasifikasikan emosi berdasarkan data twitter. Selain itu, sifat dari data twitter yang dapat memiliki banyak label emosi (***multi-class***). Manusia memiliki berbagai emosi dan sulit untuk mengumpulkan data yang cukup untuk setiap emosi. Oleh karena itu, masalah ketidakseimbangan kelas akan muncul (***class imbalance***). Pada Ujian Tengah Semester (UTS) kali ini, Anda telah disediakan dataset teks twitter yang sudah memiliki label untuk beberapa kelas emosi. Tugas utama Anda adalah membuat model yang mumpuni untuk kebutuhan klasifikasi emosi berdasarkan teks.\n\n### Informasi Data\n\nDataset yang akan digunakan adalah ****tweet_emotion.csv***. Berikut merupakan informasi tentang dataset yang dapat membantu Anda.\n\n- Total data: 40000 data\n- Label emosi: anger, boredom, empty, enthusiasm, fun, happiness, hate, love, neutral, relief, sadness, surprise, worry\n- Jumlah data untuk setiap label tidak sama (***class imbalance***)\n- Terdapat 3 kolom = 'tweet_id', 'sentiment', 'content'\n\n### Penilaian UTS\n\nUTS akan dinilai berdasaarkan 4 proses yang akan Anda lakukan, yaitu pra pengolahan data, ektraksi fitur, pembuatan model machine learning, dan evaluasi.\n\n#### Pra Pengolahan Data\n\n> **Perhatian**\n> \n> Sebelum Anda melakukan sesuatu terhadap data Anda, pastikan data yang Anda miliki sudah \"baik\", bebas dari data yang hilang, menggunakan tipe data yang sesuai, dan sebagainya.\n>\n\nData tweeter yang ada dapatkan merupakan sebuah data mentah, maka beberapa hal dapat Anda lakukan (namun tidak terbatas pada) yaitu,\n\n1. Case Folding\n2. Tokenizing\n3. Filtering\n4. Stemming\n\n*CATATAN: PADA DATA TWITTER TERDAPAT *MENTION* (@something) YANG ANDA HARUS TANGANI SEBELUM MASUK KE TAHAP EKSTRAKSI FITUR*\n\n#### Ekstrasi Fitur\n\nAnda dapat menggunakan beberapa metode, diantaranya\n\n1. Bag of Words (Count / TF-IDF)\n2. N-gram\n3. dan sebagainya\n\n#### Pembuatan Model\n\nAnda dibebaskan dalam memilih algoritma klasifikasi. Anda dapat menggunakan algoritma yang telah diajarkan didalam kelas atau yang lain, namun dengan catatan. Berdasarkan asas akuntabilitas pada pengembangan model machine learning, Anda harus dapat menjelaskan bagaimana model Anda dapat menghasilkan nilai tertentu.\n\n#### Evaluasi\n\nPada proses evaluasi, minimal Anda harus menggunakan metric akurasi. Akan tetapi Anda juga dapat menambahkan metric lain seperti Recall, Precision, F1-Score, detail Confussion Metric, ataupun Area Under Curve (AUC).","metadata":{}},{"cell_type":"markdown","source":"### Lembar Pengerjaan\nLembar pengerjaan dimulai dari cell dibawah ini","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2022-11-07T08:52:46.013305Z","iopub.execute_input":"2022-11-07T08:52:46.014723Z","iopub.status.idle":"2022-11-07T08:52:46.055889Z","shell.execute_reply.started":"2022-11-07T08:52:46.014608Z","shell.execute_reply":"2022-11-07T08:52:46.054654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pre Processing Data","metadata":{}},{"cell_type":"code","source":"# Import Library yang akan digunakan\nimport numpy as np\nimport pandas as pd\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import Data set yang akan digunakan\ndf = pd.read_csv('/kaggle/input/emotion-detection-from-text/tweet_emotions.csv')\n\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cek jumlah data yang dimiliki masing masing sentiment\ndf['sentiment'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['sentiment'].value_counts().plot(kind='bar')\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Case Folding & Cleaning Data","metadata":{}},{"cell_type":"code","source":"# function to clean the tweets \nimport re\ndef cleanTxt(text):\n    text = re.sub(r'@\\w+|#\\w+', '', text) #Removing @mentions\n    text = re.sub('#', '', text) # Removing '#' hash tag\n    text = re.sub('RT[\\s]+', '', text) # Removing RT\n    text = re.sub('\\w+:\\/\\/\\S+', '', text) # Removing hyperlink\n    #remove punctuation\n    text = re.sub('[^a-zA-Z]', ' ', text)\n    text.lower()\n    return text","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# case folding\ntemp = df['content'].str.lower()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove hashtag and mention using regex\nimport re\n\ntemp = temp.apply(lambda x: re.sub(r'@\\w+|#\\w+', '', x))\n\n# temp","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove url using regex\ntemp = temp.apply(lambda x: re.sub(r'http\\S+', '', x))\n\n# temp","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove punctuation\nimport string\n\ntemp = temp.apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n\n# temp","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove duplicate item\ntemp = temp.drop_duplicates()\n\n# temp","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove number\ntemp = temp.apply(lambda x: re.sub(r'\\d+', '', x))\n\n# temp","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check null value\ntemp.isnull().sum()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove whitespace\ntemp = temp.apply(lambda x: x.strip())\n\n# temp\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Stopwords Removal","metadata":{}},{"cell_type":"code","source":"import nltk\n# download all \n\nnltk.download('all')\n\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# remove stopword\nfrom nltk.corpus import stopwords\n\nstop = stopwords.words('english')\n\ntemp = temp.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n\n# temp","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Lemmatization","metadata":{}},{"cell_type":"code","source":"# lemmatization\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\n\ntemp = temp.apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n\n# temp","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Tokenization","metadata":{}},{"cell_type":"code","source":"# tokenization\nfrom nltk.tokenize import word_tokenize\n\ntemp = temp.apply(lambda x: word_tokenize(x))\n\n# temp","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# temp to df content_token\ndf['content_token'] = temp\n\n# df\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove NaN data in content_token\ndf = df.dropna(subset=['content_token'])\n\n# df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Finding & Removing Duplicate Synonim?","metadata":{}},{"cell_type":"code","source":"## Find synonym of each token.\nfrom nltk.corpus import wordnet\ndef find_synonym(word):\n    synonyms = []\n    for syn in wordnet.synsets(word):\n        for l in syn.lemmas():\n            synonyms.append(l.name())\n    return synonyms\n\ndf['synonym'] = df['content_token'].apply(lambda x: [find_synonym(word) for word in x])\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Dictionary of word index\nindex_word = {}\nfor i, word in enumerate(df['content_token'].sum()):\n    if word not in index_word:\n        index_word[i] = word\n        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words = [value for key, value in index_word.items()]\n\n# words","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set synonym dictionary using find_synonym function \nsynonym_dict = {}\nfor word in words:\n    synonym_dict.update({word : tuple([w.lower() for w in find_synonym(word)])})\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# synonym_dict","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove duplicate synonym\nfor key, value in synonym_dict.items():\n    synonym_dict[key] = tuple(set(value))\n\n# synonym_dict","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove null value in synonym_dict\nsynonym_dict = {k: v for k, v in synonym_dict.items() if v}\n\n# synonym_dict","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import collections\nvalue_occurrences = collections.Counter(synonym_dict.values())\n\nfiltered_synonym = {key: value for key, value in synonym_dict.items() if value_occurrences[value] == 1}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# filtered_synonym","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Function for augmenting data by replacing words with synonym using spaCy\nimport re\nimport random\nsr = random.SystemRandom()\nsplit_pattern = re.compile(r'\\s+')\ndef data_augmentation(message, aug_range=1) :\n    augmented_messages = []\n    for j in range(0,aug_range) :\n        new_message = \"\"\n        for i in filter(None, split_pattern.split(message)) :\n            new_message = new_message + \" \" + sr.choice(filtered_synonym.get(i,[i]))\n        augmented_messages.append(new_message)\n    return augmented_messages\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweet_count = df.sentiment.value_counts().to_dict()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Get max intent count to match other minority classes through data augmentation\nimport operator\nmax_intent_count = max(tweet_count.items(), key=operator.itemgetter(1))[1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Balancing Data\n\nDikarenakan data sentiment terpaut sangat jauh, seperti neutral berisikan 8638 data, sedangkan anger sebanyak 110 data. Kami memutuskan untuk balancing data agar membuat data lebih adil dalam hal pembelajaran akurasi nantinya. Kami melakukan metode Oversampling yang artinya melakukan penambahan data sintetis yang merujuk kepada jumlah data terbanyak dalam dataset.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport math\nimport tqdm\nnewdf = pd.DataFrame()\nfor intent, count in tweet_count.items() :\n    count_diff = max_intent_count - count    ## Difference to fill\n    multiplication_count = math.ceil((count_diff)/count)  ## Multiplying a minority classes for multiplication_count times\n    if (multiplication_count) :\n        old_message_df = pd.DataFrame()\n        new_message_df = pd.DataFrame()\n        for message in tqdm.tqdm(df[df[\"sentiment\"] == intent]['content'].values) :\n            ## Extracting existing minority class batch\n            dummy1 = pd.DataFrame([message], columns=['content'])\n            dummy1[\"sentiment\"] = intent\n            # concat existing minority class batch\n            old_message_df = pd.concat([old_message_df, dummy1])\n\n            ## Creating new augmented batch from existing minority class\n            new_messages = data_augmentation(message,  multiplication_count)\n            dummy2 = pd.DataFrame(new_messages, columns=['content'])\n            dummy2[\"sentiment\"] = intent\n            # concat new augmented batch\n            new_message_df = pd.concat([new_message_df, dummy2])\n\n        ## Select random data points from augmented data\n        new_message_df=new_message_df.take(np.random.permutation(len(new_message_df))[:count_diff])\n        \n        ## Merge existing and augmented data points using concat\n        newdf = pd.concat([newdf, old_message_df, new_message_df])\n        # newdf = newdf.append([old_message_df,new_message_df])\n    else :\n        newdf = pd.concat([newdf, df[df[\"sentiment\"] == intent]])\n        # newdf = newdf.append(df[df[\"Intent\"] == intent])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newdf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nnewdf.value_counts('sentiment')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Save newdf to csv file\nnewdf.to_csv('data/augmented_data.csv', index=False)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_df = pd.read_csv('data/augmented_data.csv')\n\nclean_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_df.value_counts('sentiment')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_df['sentiment'].value_counts().plot(kind='bar')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cleaning the tweets using clean_tweet function\n\nclean_df['clean_tweet'] = clean_df['content'].apply(lambda x: cleanTxt(x))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lower casing clean_tweet column\n\nclean_df['clean_tweet'] = clean_df['clean_tweet'].apply(lambda x: x.lower())\n\n# clean_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to remove stop words from clean_tweet column\n\ndef remove_stopwords(text):\n    text = [word for word in text.split() if word not in stop]\n    return \" \".join(text)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# stopword removal\n\nclean_df['clean_tweet'] = clean_df['clean_tweet'].apply(lambda x: remove_stopwords(x))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to lemmitize clean_tweet column\n\ndef lemmatization(text):\n    text = [lemmatizer.lemmatize(word) for word in text.split()]\n    return \" \".join(text)\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lemmitization\n\nclean_df['clean_tweet'] = clean_df['clean_tweet'].apply(lambda x: lemmatization(x))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenization using word_tokenize\n\nclean_df['clean_tweet_token'] = clean_df['clean_tweet'].apply(lambda x: word_tokenize(x))\n\n# clean_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ekstraksi Fitur","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# split data into train and test\n\n\nX_train, X_test, y_train, y_test = train_test_split(clean_df['clean_tweet_token'], clean_df['sentiment'], test_size=0.2, random_state=42)\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### TF-IDF","metadata":{}},{"cell_type":"code","source":"\n# vectorization\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer()\n\nX_train = vectorizer.fit_transform(X_train.astype('U'))\n\nX_test = vectorizer.transform(X_test.astype('U'))\n\nX_train.shape, X_test.shape\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pembuatan Model","metadata":{}},{"cell_type":"markdown","source":"##### Model Multinomial Naive Bayes","metadata":{}},{"cell_type":"code","source":"# model training\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\n\n# Inisiasi MultinomialNB\nmnb = MultinomialNB()\n\n# Fit model\nmnb.fit(X_train, y_train)\n\n# Prediksi dengan data training\ny_pred_train = mnb.predict(X_train)\n\n# Evaluasi akurasi data training\nacc_train = accuracy_score(y_train, y_pred_train)\n\n# Prediksi dengan data training\ny_pred_test = mnb.predict(X_test)\n\n# Evaluasi akurasi data training\nacc_test = accuracy_score(y_test, y_pred_test)\n\n# Print hasil evaluasi\nprint(f'Hasil perhitungan akurasi Data Train : {acc_train}')\nprint(f'Hasil perhitungan akurasi Data Test  : {acc_test}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluasi","metadata":{}},{"cell_type":"code","source":"# evaluation\n\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nprint(classification_report(y_test, y_pred_test))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(confusion_matrix(y_test, y_pred_test))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Model Linear SVC","metadata":{}},{"cell_type":"code","source":"# Model using Linear SVC\n\nfrom sklearn.svm import LinearSVC\n\n# Inisiasi LinearSVC\n\nlsvc = LinearSVC()\n\n# Fit model\nlsvc.fit(X_train, y_train)\n\n# Prediksi dengan data training\n\ny_pred_train = lsvc.predict(X_train)\n\n# Evaluasi akurasi data training\n\nacc_train = accuracy_score(y_train, y_pred_train)\n\n# Prediksi dengan data training\n\ny_pred_test = lsvc.predict(X_test)\n\n# Evaluasi akurasi data training\n\nacc_test = accuracy_score(y_test, y_pred_test)\n\n# Print hasil evaluasi\n\nprint(f'Hasil perhitungan akurasi Data Train : {acc_train}')\n\nprint(f'Hasil perhitungan akurasi Data Test  : {acc_test}')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluation\n\nprint(classification_report(y_test, y_pred_test))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(confusion_matrix(y_test, y_pred_test))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model using Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Inisiasi LogisticRegression\n\nlr = LogisticRegression()\n\n# Fit model\n\nlr.fit(X_train, y_train)\n\n# Prediksi dengan data training\n\ny_pred_train = lr.predict(X_train)\n\n# Evaluasi akurasi data training\n\nacc_train = accuracy_score(y_train, y_pred_train)\n\n# Prediksi dengan data training\n\ny_pred_test = lr.predict(X_test)\n\n# Evaluasi akurasi data training\n\nacc_test = accuracy_score(y_test, y_pred_test)\n\n# Print hasil evaluasi\n\nprint(f'Hasil perhitungan akurasi Data Train : {acc_train}')\n\nprint(f'Hasil perhitungan akurasi Data Test  : {acc_test}')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Kesimpulan\n\nDari ke 3 plain model di atas, Kami menemukan bahwa akurasi terbaik didapatkan ketika menggunakan model Linear SVC dengan keakurasian<br />\n```Hasil perhitungan akurasi Data Train : 0.8640307472785437``` <br />\n```Hasil perhitungan akurasi Data Test  : 0.6568627450980392```<br />\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}