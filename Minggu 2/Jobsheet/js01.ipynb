{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create manual normalization feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_feature(data):\n",
    "    n_max = max(data)\n",
    "    n_min = min(data)\n",
    "    len_of_data = len(data)\n",
    "    for i in range(len_of_data):\n",
    "        data[i] = (data[i] - n_min) / (n_max - n_min)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.375, 0.125, 0.5, 1.0]\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    10, 25, 15, 30, 50\n",
    "]\n",
    "\n",
    "print(norm_feature(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalisasi with SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100, 0.001], [8, 0.05], [50, 0.005], [88, 0.07], [4, 0.1]]\n",
      "[[1.0e+02 1.0e-03]\n",
      " [8.0e+00 5.0e-02]\n",
      " [5.0e+01 5.0e-03]\n",
      " [8.8e+01 7.0e-02]\n",
      " [4.0e+00 1.0e-01]]\n",
      "[[1.         0.        ]\n",
      " [0.04166667 0.49494949]\n",
      " [0.47916667 0.04040404]\n",
      " [0.875      0.6969697 ]\n",
      " [0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = [ \n",
    "    [100, 0.001],\n",
    "    [8, 0.05],\n",
    "    [50, 0.005],\n",
    "    [88, 0.07],\n",
    "    [4, 0.1],\n",
    "]\n",
    "\n",
    "print(data)\n",
    "\n",
    "print(numpy.asarray(data))\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaled = scaler.fit_transform(numpy.asarray(data))\n",
    "print(scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean, pstdev\n",
    "\n",
    "\n",
    "def std_feature(data):\n",
    "    kolom_data = data.shape[1]\n",
    "    baris_data = data.shape[0]\n",
    "\n",
    "    for i in range(0, baris_data):\n",
    "        for j in range(0,kolom_data):\n",
    "            mean_data = mean(data[:,j])\n",
    "            std_data = pstdev(data[:,j])\n",
    "            data[i][j] = (data[i][j] - mean_data) / std_data \n",
    "            \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.26398112 -1.16389967]\n",
      " [-0.65623116  0.48622176]\n",
      " [ 0.6102111   0.18921441]\n",
      " [ 1.99807462  0.23476602]\n",
      " [ 1.65186847  0.22506085]]\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    [100, 0.001],\n",
    "    [8, 0.05],\n",
    "    [50, 0.005],\n",
    "    [88, 0.07],\n",
    "    [4, 0.1],\n",
    "]\n",
    "\n",
    "data = numpy.asarray(data)\n",
    "\n",
    "print(std_feature(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardisasi with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100, 0.001], [8, 0.05], [50, 0.005], [88, 0.07], [4, 0.1]]\n",
      "[[ 1.26398112 -1.16389967]\n",
      " [-1.06174414  0.12639634]\n",
      " [ 0.         -1.05856939]\n",
      " [ 0.96062565  0.65304778]\n",
      " [-1.16286263  1.44302493]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import asarray\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = [\n",
    "    [100, 0.001],\n",
    "    [8, 0.05],\n",
    "    [50, 0.005],\n",
    "    [88, 0.07],\n",
    "    [4, 0.1],\n",
    "]\n",
    "\n",
    "print(data)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaled = scaler.fit_transform(data)\n",
    "print(scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction dari Data Kategorik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "data = [\n",
    "    ['POLINEMA'],\n",
    "    ['PENS'], \n",
    "    ['PNJ'], \n",
    "    ['PNP'],\n",
    "    ['POLBAN']\n",
    "]\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "encoded = encoder.fit_transform(data)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "data = [\n",
    "    ['POLINEMA'],\n",
    "    ['PENS'],\n",
    "    ['PNJ'],\n",
    "    ['PNP'],\n",
    "    ['POLBAN']\n",
    "]\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoded = encoder.fit_transform(data)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction pada Data Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ate' 'away' 'cat' 'end' 'finally' 'house' 'little' 'mouse' 'ran' 'saw'\n",
      " 'story' 'tiny']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.4755751 , 0.58946308, 0.28088232, 0.        , 0.        ,\n",
       "         0.        , 0.58946308],\n",
       "        [0.        , 0.        , 0.58873218, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.34771471, 0.        , 0.72971837,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.58946308, 0.        , 0.        , 0.        ,\n",
       "         0.4755751 , 0.        , 0.28088232, 0.58946308, 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.58946308, 0.        , 0.4755751 , 0.        , 0.58946308,\n",
       "         0.        , 0.        , 0.28088232, 0.        , 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.67009179, 0.        ,\n",
       "         0.        , 0.        , 0.31930233, 0.        , 0.        ,\n",
       "         0.67009179, 0.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'the house had a tiny little mouse',\n",
    "    'the cat saw the mouse', \n",
    "    'the mouse ran away from the house',\n",
    "    'the cat finally ate the mouse', \n",
    "    'the end of the mouse story'\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words ='english')\n",
    "data = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "# print(data.todense())\n",
    "data.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000\n",
      "10\n",
      "12th\n",
      "14\n",
      "1869\n",
      "1870\n",
      "1878\n",
      "1883\n",
      "1oth\n",
      "2d\n",
      "313\n",
      "4th\n",
      "5024\n",
      "5517\n",
      "558\n",
      "7th\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "87\n",
      "90\n",
      "9th\n",
      "abjur\n",
      "abl\n",
      "abound\n",
      "absenc\n",
      "absolut\n",
      "abus\n",
      "accid\n",
      "accident\n",
      "accid\n",
      "accomplish\n",
      "account\n",
      "accur\n",
      "accus\n",
      "act\n",
      "address\n",
      "ado\n",
      "advantag\n",
      "adventur\n",
      "advic\n",
      "advis\n",
      "affair\n",
      "afraid\n",
      "agit\n",
      "ago\n",
      "ah\n",
      "aid\n",
      "air\n",
      "alarm\n",
      "albert\n",
      "allow\n",
      "amateur\n",
      "america\n",
      "american\n",
      "amid\n",
      "analysi\n",
      "analyt\n",
      "anatomi\n",
      "anderson\n",
      "angri\n",
      "anim\n",
      "answer\n",
      "anxieti\n",
      "anxious\n",
      "apolog\n",
      "appeal\n",
      "appear\n",
      "archiv\n",
      "argu\n",
      "arm\n",
      "armi\n",
      "arriv\n",
      "art\n",
      "ari\n",
      "ascend\n",
      "ash\n",
      "ask\n",
      "assert\n",
      "assum\n",
      "astronomi\n",
      "atlant\n",
      "attain\n",
      "attent\n",
      "attic\n",
      "attract\n",
      "augustin\n",
      "author\n",
      "avers\n",
      "await\n",
      "away\n",
      "aw\n",
      "backgammon\n",
      "badg\n",
      "badli\n",
      "baffl\n",
      "bag\n",
      "baker\n",
      "bank\n",
      "bar\n",
      "bark\n",
      "bar\n",
      "basin\n",
      "bb\n",
      "bear\n",
      "beast\n",
      "beaten\n",
      "bed\n",
      "beg\n",
      "began\n",
      "beg\n",
      "begin\n",
      "begun\n",
      "believ\n",
      "bell\n",
      "belong\n",
      "bend\n",
      "beneath\n",
      "bent\n",
      "best\n",
      "better\n",
      "bicycl\n",
      "bill\n",
      "bite\n",
      "black\n",
      "blanc\n",
      "blaze\n",
      "blend\n",
      "blow\n",
      "blown\n",
      "blue\n",
      "board\n",
      "boat\n",
      "bodi\n",
      "bone\n",
      "book\n",
      "born\n",
      "botani\n",
      "bound\n",
      "box\n",
      "boxer\n",
      "boy\n",
      "brain\n",
      "branc\n",
      "branch\n",
      "brandi\n",
      "brass\n",
      "brave\n",
      "brazen\n",
      "breakfast\n",
      "break\n",
      "bridg\n",
      "brightli\n",
      "bright\n",
      "bring\n",
      "british\n",
      "brother\n",
      "brought\n",
      "bull\n",
      "bundl\n",
      "burn\n",
      "burst\n",
      "busi\n",
      "cabl\n",
      "cage\n",
      "calhoun\n",
      "call\n",
      "calmli\n",
      "camberwel\n",
      "came\n",
      "cap\n",
      "captain\n",
      "card\n",
      "care\n",
      "career\n",
      "care\n",
      "cargo\n",
      "carolina\n",
      "carpet\n",
      "carri\n",
      "carv\n",
      "case\n",
      "cashbox\n",
      "catch\n",
      "caught\n",
      "caus\n",
      "caution\n",
      "ceas\n",
      "ceil\n",
      "certain\n",
      "certainli\n",
      "certainti\n",
      "chain\n",
      "chair\n",
      "chalk\n",
      "chamber\n",
      "chanc\n",
      "chang\n",
      "charg\n",
      "charm\n",
      "chase\n",
      "cheat\n",
      "checkmat\n",
      "cheek\n",
      "chemistri\n",
      "child\n",
      "chill\n",
      "chimney\n",
      "choos\n",
      "chuck\n",
      "ciousli\n",
      "circum\n",
      "citi\n",
      "civil\n",
      "clad\n",
      "clark\n",
      "clasp\n",
      "class\n",
      "clay\n",
      "clear\n",
      "clench\n",
      "client\n",
      "climat\n",
      "clock\n",
      "close\n",
      "club\n",
      "clue\n",
      "coat\n",
      "cocain\n",
      "cock\n",
      "coffe\n",
      "coincid\n",
      "cold\n",
      "collaps\n",
      "collect\n",
      "colonel\n",
      "colour\n",
      "come\n",
      "comfort\n",
      "come\n",
      "command\n",
      "commenc\n",
      "common\n",
      "commun\n",
      "companion\n",
      "compar\n",
      "compet\n",
      "concept\n",
      "concern\n",
      "condi\n",
      "confeder\n",
      "conjectur\n",
      "connect\n",
      "conscious\n",
      "consid\n",
      "consider\n",
      "consid\n",
      "constabl\n",
      "contempl\n",
      "content\n",
      "continu\n",
      "convict\n",
      "convinc\n",
      "cook\n",
      "coop\n",
      "coron\n",
      "correctli\n",
      "count\n",
      "countri\n",
      "courag\n",
      "cours\n",
      "court\n",
      "coventri\n",
      "cover\n",
      "credit\n",
      "cri\n",
      "crime\n",
      "croni\n",
      "cross\n",
      "crowd\n",
      "crumpl\n",
      "cun\n",
      "cup\n",
      "cupboard\n",
      "curios\n",
      "curs\n",
      "cuvier\n",
      "danger\n",
      "dark\n",
      "date\n",
      "day\n",
      "dead\n",
      "deadliest\n",
      "deadli\n",
      "deal\n",
      "dear\n",
      "death\n",
      "deceas\n",
      "deceiv\n",
      "decoy\n",
      "deduc\n",
      "deduct\n",
      "deed\n",
      "deep\n",
      "deepest\n",
      "deepli\n",
      "defin\n",
      "definit\n",
      "degre\n",
      "delay\n",
      "delicaci\n",
      "depend\n",
      "depress\n",
      "deriv\n",
      "descend\n",
      "describ\n",
      "despair\n",
      "destroy\n",
      "destruct\n",
      "detail\n",
      "determin\n",
      "devil\n",
      "devilish\n",
      "devil\n",
      "devour\n",
      "diari\n",
      "did\n",
      "didn\n",
      "differ\n",
      "dim\n",
      "direct\n",
      "disadvantag\n",
      "disappear\n",
      "discolour\n",
      "discuss\n",
      "dislik\n",
      "dispos\n",
      "disposit\n",
      "distanc\n",
      "distinct\n",
      "disturb\n",
      "divis\n",
      "dock\n",
      "doctor\n",
      "document\n",
      "doe\n",
      "door\n",
      "doubt\n",
      "downward\n",
      "drank\n",
      "draught\n",
      "draw\n",
      "dread\n",
      "dreamland\n",
      "drew\n",
      "dri\n",
      "drive\n",
      "drove\n",
      "drunken\n",
      "dri\n",
      "dunde\n",
      "duti\n",
      "dweller\n",
      "earli\n",
      "earth\n",
      "eas\n",
      "easili\n",
      "east\n",
      "easterli\n",
      "eastern\n",
      "easi\n",
      "eccentr\n",
      "edg\n",
      "educ\n",
      "effect\n",
      "effort\n",
      "elaps\n",
      "elbow\n",
      "element\n",
      "elia\n",
      "embank\n",
      "emerg\n",
      "emigr\n",
      "encourag\n",
      "encyclopaedia\n",
      "end\n",
      "endeav\n",
      "end\n",
      "enemi\n",
      "energi\n",
      "engag\n",
      "england\n",
      "english\n",
      "enigmat\n",
      "enjoy\n",
      "enlarg\n",
      "enter\n",
      "entir\n",
      "entri\n",
      "env\n",
      "envelop\n",
      "equinocti\n",
      "ere\n",
      "error\n",
      "er\n",
      "escap\n",
      "essenti\n",
      "estat\n",
      "europ\n",
      "even\n",
      "event\n",
      "eventu\n",
      "evid\n",
      "evil\n",
      "ex\n",
      "examin\n",
      "excel\n",
      "except\n",
      "exchang\n",
      "exclaim\n",
      "excus\n",
      "exercis\n",
      "exhibit\n",
      "expect\n",
      "experi\n",
      "explan\n",
      "extend\n",
      "extrem\n",
      "eye\n",
      "face\n",
      "fact\n",
      "factori\n",
      "fact\n",
      "fair\n",
      "fallen\n",
      "famili\n",
      "fanci\n",
      "fantast\n",
      "far\n",
      "fareham\n",
      "farther\n",
      "fashion\n",
      "fate\n",
      "father\n",
      "favour\n",
      "fear\n",
      "featur\n",
      "febru\n",
      "feel\n",
      "feet\n",
      "fell\n",
      "fellowmen\n",
      "felt\n",
      "field\n",
      "fierc\n",
      "file\n",
      "final\n",
      "fine\n",
      "finger\n",
      "finn\n",
      "fireplac\n",
      "fit\n",
      "flap\n",
      "flaw\n",
      "fli\n",
      "floor\n",
      "florida\n",
      "flourish\n",
      "fluffi\n",
      "flush\n",
      "flutter\n",
      "fli\n",
      "fold\n",
      "follow\n",
      "fond\n",
      "foot\n",
      "footmark\n",
      "forbid\n",
      "forc\n",
      "fordham\n",
      "forebod\n",
      "foreign\n",
      "forese\n",
      "foresight\n",
      "forget\n",
      "form\n",
      "formid\n",
      "fort\n",
      "fortun\n",
      "forward\n",
      "fought\n",
      "foul\n",
      "found\n",
      "fourth\n",
      "franchis\n",
      "free\n",
      "freebodi\n",
      "frenzi\n",
      "fresh\n",
      "friend\n",
      "friendship\n",
      "fulfil\n",
      "furnish\n",
      "furnitur\n",
      "fuss\n",
      "futur\n",
      "gain\n",
      "gale\n",
      "gang\n",
      "garden\n",
      "gener\n",
      "gentleman\n",
      "gentlemen\n",
      "geolog\n",
      "georgia\n",
      "german\n",
      "given\n",
      "glad\n",
      "glanc\n",
      "glare\n",
      "glisten\n",
      "glow\n",
      "gm\n",
      "god\n",
      "go\n",
      "golden\n",
      "gone\n",
      "good\n",
      "goodwin\n",
      "got\n",
      "govern\n",
      "grandfath\n",
      "grasp\n",
      "grate\n",
      "gravesend\n",
      "great\n",
      "greater\n",
      "greatest\n",
      "green\n",
      "grew\n",
      "grice\n",
      "grin\n",
      "grip\n",
      "groom\n",
      "guard\n",
      "guilti\n",
      "gum\n",
      "habit\n",
      "half\n",
      "hand\n",
      "handsom\n",
      "hang\n",
      "happen\n",
      "happili\n",
      "hard\n",
      "hardli\n",
      "hast\n",
      "have\n",
      "head\n",
      "health\n",
      "hear\n",
      "heard\n",
      "heart\n",
      "heavili\n",
      "heavi\n",
      "held\n",
      "help\n",
      "helpless\n",
      "help\n",
      "hereditari\n",
      "hesit\n",
      "high\n",
      "higher\n",
      "highest\n",
      "hill\n",
      "holder\n",
      "hollow\n",
      "holm\n",
      "home\n",
      "homeward\n",
      "hood\n",
      "hook\n",
      "hope\n",
      "horror\n",
      "horsham\n",
      "hot\n",
      "hour\n",
      "hous\n",
      "howl\n",
      "hudson\n",
      "human\n",
      "hungri\n",
      "hurri\n",
      "hurriedli\n",
      "hurri\n",
      "hurt\n",
      "hypothesi\n",
      "idea\n",
      "ideal\n",
      "i\n",
      "illustr\n",
      "imbecil\n",
      "immin\n",
      "implac\n",
      "implic\n",
      "impli\n",
      "implor\n",
      "import\n",
      "imposs\n",
      "impress\n",
      "impun\n",
      "incid\n",
      "inclin\n",
      "incred\n",
      "index\n",
      "india\n",
      "indic\n",
      "individu\n",
      "inexor\n",
      "inexplic\n",
      "inform\n",
      "ing\n",
      "inherit\n",
      "initi\n",
      "ink\n",
      "inner\n",
      "inquir\n",
      "insid\n",
      "inspector\n",
      "instant\n",
      "instantli\n",
      "interest\n",
      "interpos\n",
      "intrud\n",
      "invari\n",
      "invent\n",
      "island\n",
      "isl\n",
      "item\n",
      "jackson\n",
      "jame\n",
      "januari\n",
      "john\n",
      "joke\n",
      "joseph\n",
      "jump\n",
      "junki\n",
      "juri\n",
      "just\n",
      "keen\n",
      "kept\n",
      "key\n",
      "keyhol\n",
      "key\n",
      "kind\n",
      "kindli\n",
      "klan\n",
      "klux\n",
      "knee\n",
      "knew\n",
      "know\n",
      "knowledg\n",
      "known\n",
      "ku\n",
      "label\n",
      "laid\n",
      "lamp\n",
      "land\n",
      "landladi\n",
      "larg\n",
      "late\n",
      "later\n",
      "laugh\n",
      "law\n",
      "lawyer\n",
      "lay\n",
      "lead\n",
      "leader\n",
      "lean\n",
      "learn\n",
      "leav\n",
      "led\n",
      "lee\n",
      "left\n",
      "lengthen\n",
      "let\n",
      "letter\n",
      "librari\n",
      "lid\n",
      "lie\n",
      "life\n",
      "lift\n",
      "light\n",
      "like\n",
      "limit\n",
      "line\n",
      "ling\n",
      "link\n",
      "lip\n",
      "li\n",
      "listen\n",
      "lit\n",
      "literatur\n",
      "littl\n",
      "live\n",
      "ll\n",
      "lloyd\n",
      "load\n",
      "loaf\n",
      "local\n",
      "lock\n",
      "logic\n",
      "london\n",
      "lone\n",
      "long\n",
      "longer\n",
      "look\n",
      "lope\n",
      "lose\n",
      "loss\n",
      "lost\n",
      "louder\n",
      "louisiana\n",
      "love\n",
      "lower\n",
      "lumber\n",
      "luxuri\n",
      "lie\n",
      "mad\n",
      "maid\n",
      "mail\n",
      "main\n",
      "major\n",
      "make\n",
      "man\n",
      "mankind\n",
      "manner\n",
      "march\n",
      "margin\n",
      "mark\n",
      "mari\n",
      "mass\n",
      "master\n",
      "mate\n",
      "matter\n",
      "mccauley\n",
      "mean\n",
      "meantim\n",
      "meet\n",
      "melon\n",
      "memoranda\n",
      "memori\n",
      "men\n",
      "mendic\n",
      "ment\n",
      "mention\n",
      "messag\n",
      "met\n",
      "method\n",
      "mile\n",
      "mind\n",
      "minut\n",
      "mise\n",
      "miser\n",
      "miss\n",
      "mission\n",
      "mixtur\n",
      "moistur\n",
      "moment\n",
      "money\n",
      "month\n",
      "moodili\n",
      "morn\n",
      "morrow\n",
      "mother\n",
      "mouth\n",
      "move\n",
      "movement\n",
      "mr\n",
      "mud\n",
      "murder\n",
      "muster\n",
      "mysteri\n",
      "nah\n",
      "name\n",
      "narr\n",
      "nativ\n",
      "natur\n",
      "near\n",
      "nearli\n",
      "necessari\n",
      "need\n",
      "negro\n",
      "neighbourhood\n",
      "nervou\n",
      "new\n",
      "newcom\n",
      "newli\n",
      "news\n",
      "newspap\n",
      "nez\n",
      "nigh\n",
      "night\n",
      "nonsens\n",
      "north\n",
      "notabl\n",
      "note\n",
      "notic\n",
      "number\n",
      "oak\n",
      "oath\n",
      "object\n",
      "observ\n",
      "obstin\n",
      "obvious\n",
      "occas\n",
      "offer\n",
      "oh\n",
      "old\n",
      "one\n",
      "open\n",
      "openli\n",
      "openshaw\n",
      "opinion\n",
      "oppos\n",
      "orang\n",
      "order\n",
      "ordinari\n",
      "organ\n",
      "origin\n",
      "our\n",
      "outbreak\n",
      "outrag\n",
      "outsid\n",
      "outstretch\n",
      "overcoat\n",
      "overtaken\n",
      "owe\n",
      "pace\n",
      "page\n",
      "paid\n",
      "pain\n",
      "pale\n",
      "palm\n",
      "palpit\n",
      "paper\n",
      "paradol\n",
      "paramor\n",
      "partial\n",
      "particular\n",
      "parti\n",
      "part\n",
      "passag\n",
      "pass\n",
      "passer\n",
      "past\n",
      "pat\n",
      "patente\n",
      "paterson\n",
      "path\n",
      "patienc\n",
      "patter\n",
      "peac\n",
      "peculiar\n",
      "peep\n",
      "pen\n",
      "perfect\n",
      "peril\n",
      "permit\n",
      "perpetr\n",
      "persecut\n",
      "person\n",
      "persuad\n",
      "petti\n",
      "philosophi\n",
      "piec\n",
      "pinc\n",
      "pipe\n",
      "pip\n",
      "pit\n",
      "pitch\n",
      "pit\n",
      "place\n",
      "plan\n",
      "plantat\n",
      "planter\n",
      "plate\n",
      "platform\n",
      "player\n",
      "play\n",
      "plot\n",
      "pocket\n",
      "point\n",
      "poison\n",
      "polic\n",
      "policeman\n",
      "polici\n",
      "polit\n",
      "politician\n",
      "polit\n",
      "ponder\n",
      "pondicherri\n",
      "pooh\n",
      "pool\n",
      "poor\n",
      "port\n",
      "portsdown\n",
      "possess\n",
      "possibl\n",
      "post\n",
      "postmark\n",
      "pound\n",
      "power\n",
      "practic\n",
      "pray\n",
      "precaut\n",
      "preced\n",
      "precis\n",
      "precursor\n",
      "prendergast\n",
      "preposter\n",
      "presenc\n",
      "present\n",
      "press\n",
      "presum\n",
      "presumpt\n",
      "pride\n",
      "princip\n",
      "print\n",
      "privaci\n",
      "privat\n",
      "probabl\n",
      "problem\n",
      "proceed\n",
      "produc\n",
      "profession\n",
      "profound\n",
      "proof\n",
      "properti\n",
      "protrud\n",
      "prove\n",
      "provinci\n",
      "public\n",
      "pull\n",
      "punish\n",
      "purpos\n",
      "pursu\n",
      "push\n",
      "putti\n",
      "puzzl\n",
      "qualiti\n",
      "quarter\n",
      "question\n",
      "quick\n",
      "quickli\n",
      "quit\n",
      "rabbit\n",
      "radic\n",
      "rag\n",
      "rain\n",
      "rais\n",
      "ran\n",
      "rapidli\n",
      "rare\n",
      "rave\n",
      "rea\n",
      "reabsorb\n",
      "reach\n",
      "read\n",
      "readili\n",
      "readi\n",
      "real\n",
      "realli\n",
      "reason\n",
      "receipt\n",
      "receiv\n",
      "recept\n",
      "recogn\n",
      "reconstruct\n",
      "record\n",
      "recov\n",
      "red\n",
      "refin\n",
      "regard\n",
      "region\n",
      "regi\n",
      "regist\n",
      "relat\n",
      "relentless\n",
      "remain\n",
      "remark\n",
      "remem\n",
      "rememb\n",
      "remov\n",
      "repeat\n",
      "report\n",
      "repres\n",
      "republican\n",
      "reput\n",
      "request\n",
      "rescu\n",
      "resem\n",
      "resid\n",
      "resistless\n",
      "resolut\n",
      "resourc\n",
      "rest\n",
      "result\n",
      "retain\n",
      "retir\n",
      "return\n",
      "reveng\n",
      "revolv\n",
      "rifl\n",
      "right\n",
      "rightli\n",
      "ring\n",
      "rise\n",
      "river\n",
      "riversid\n",
      "road\n",
      "robberi\n",
      "room\n",
      "root\n",
      "rose\n",
      "round\n",
      "routin\n",
      "rpgnet\n",
      "rummag\n",
      "run\n",
      "rush\n",
      "russel\n",
      "rusti\n",
      "safeti\n",
      "said\n",
      "sail\n",
      "salli\n",
      "sallow\n",
      "sank\n",
      "sat\n",
      "savan\n",
      "savannah\n",
      "save\n",
      "saw\n",
      "say\n",
      "scandal\n",
      "scare\n",
      "scatter\n",
      "scrawl\n",
      "scream\n",
      "scum\n",
      "sea\n",
      "seal\n",
      "seaport\n",
      "search\n",
      "second\n",
      "secret\n",
      "seed\n",
      "seek\n",
      "seen\n",
      "self\n",
      "sell\n",
      "send\n",
      "sender\n",
      "send\n",
      "seni\n",
      "sensat\n",
      "senseless\n",
      "sens\n",
      "sent\n",
      "septemb\n",
      "seri\n",
      "servant\n",
      "set\n",
      "seven\n",
      "sever\n",
      "shake\n",
      "shaken\n",
      "shall\n",
      "shape\n",
      "sharp\n",
      "shatter\n",
      "sheep\n",
      "sheet\n",
      "shelf\n",
      "sherlock\n",
      "shine\n",
      "ship\n",
      "sholto\n",
      "shook\n",
      "shoulder\n",
      "show\n",
      "shown\n",
      "show\n",
      "shriek\n",
      "sight\n",
      "sign\n",
      "silenc\n",
      "singl\n",
      "singular\n",
      "sinist\n",
      "sink\n",
      "sin\n",
      "sir\n",
      "sit\n",
      "site\n",
      "sit\n",
      "situat\n",
      "sixteen\n",
      "sketch\n",
      "skill\n",
      "skin\n",
      "skull\n",
      "sleep\n",
      "sleepless\n",
      "small\n",
      "smile\n",
      "smoke\n",
      "snake\n",
      "snug\n",
      "sob\n",
      "sober\n",
      "societi\n",
      "soldier\n",
      "solitud\n",
      "solut\n",
      "solv\n",
      "somewhat\n",
      "son\n",
      "soon\n",
      "sophi\n",
      "sorri\n",
      "sort\n",
      "sought\n",
      "soul\n",
      "sound\n",
      "south\n",
      "southern\n",
      "spare\n",
      "speak\n",
      "spend\n",
      "spent\n",
      "spirit\n",
      "spite\n",
      "splash\n",
      "spo\n",
      "spoke\n",
      "sprang\n",
      "sprig\n",
      "spun\n",
      "squeez\n",
      "st\n",
      "stage\n",
      "stain\n",
      "stair\n",
      "stammer\n",
      "stamp\n",
      "stanc\n",
      "stand\n",
      "star\n",
      "start\n",
      "startl\n",
      "starv\n",
      "state\n",
      "statement\n",
      "state\n",
      "station\n",
      "stay\n",
      "steamboat\n",
      "steamer\n",
      "step\n",
      "stern\n",
      "stevedor\n",
      "stock\n",
      "stood\n",
      "stori\n",
      "storm\n",
      "stormi\n",
      "stori\n",
      "strang\n",
      "stranger\n",
      "stream\n",
      "street\n",
      "stretch\n",
      "strong\n",
      "struck\n",
      "studi\n",
      "subdu\n",
      "submit\n",
      "succeed\n",
      "success\n",
      "successor\n",
      "sudden\n",
      "suddenli\n",
      "suggest\n",
      "suicid\n",
      "sun\n",
      "sundial\n",
      "sunk\n",
      "suppos\n",
      "sur\n",
      "sure\n",
      "surpris\n",
      "suspicion\n",
      "sussex\n",
      "swain\n",
      "swash\n",
      "swing\n",
      "swordsman\n",
      "systemat\n",
      "tabl\n",
      "taken\n",
      "tankervil\n",
      "tap\n",
      "tear\n",
      "telegram\n",
      "tell\n",
      "temper\n",
      "tempt\n",
      "tene\n",
      "tennesse\n",
      "tere\n",
      "terribl\n",
      "terror\n",
      "ter\n",
      "texa\n",
      "text\n",
      "thank\n",
      "their\n",
      "thing\n",
      "think\n",
      "thirti\n",
      "thoroughli\n",
      "thought\n",
      "threat\n",
      "threaten\n",
      "thrust\n",
      "tide\n",
      "time\n",
      "tint\n",
      "tion\n",
      "tip\n",
      "tire\n",
      "tobacco\n",
      "toe\n",
      "token\n",
      "told\n",
      "tomfooleri\n",
      "tonnag\n",
      "took\n",
      "torn\n",
      "touch\n",
      "town\n",
      "trace\n",
      "track\n",
      "trade\n",
      "tradespeopl\n",
      "tragedi\n",
      "train\n",
      "travel\n",
      "trebl\n",
      "trembl\n",
      "trimli\n",
      "troubl\n",
      "trough\n",
      "true\n",
      "trunk\n",
      "trust\n",
      "truth\n",
      "tri\n",
      "tumultu\n",
      "turn\n",
      "tut\n",
      "twilight\n",
      "uffa\n",
      "umbrella\n",
      "unabl\n",
      "unaveng\n",
      "unbreak\n",
      "unburn\n",
      "unclasp\n",
      "uncl\n",
      "uncontrol\n",
      "understand\n",
      "understood\n",
      "undoubtedli\n",
      "unfailingli\n",
      "unfenc\n",
      "unforeseen\n",
      "unfortun\n",
      "unhappi\n",
      "union\n",
      "uniqu\n",
      "unit\n",
      "unknown\n",
      "unopen\n",
      "unsystemat\n",
      "untam\n",
      "urg\n",
      "urgenc\n",
      "use\n",
      "usual\n",
      "util\n",
      "vacant\n",
      "vagu\n",
      "vain\n",
      "variabl\n",
      "vault\n",
      "veil\n",
      "verdict\n",
      "vessel\n",
      "victim\n",
      "view\n",
      "violenc\n",
      "violin\n",
      "visit\n",
      "visitor\n",
      "vital\n",
      "voic\n",
      "volum\n",
      "vora\n",
      "vot\n",
      "waistcoat\n",
      "wait\n",
      "walk\n",
      "want\n",
      "war\n",
      "warehous\n",
      "warn\n",
      "wash\n",
      "watch\n",
      "water\n",
      "waterloo\n",
      "waterproof\n",
      "watson\n",
      "wave\n",
      "way\n",
      "weather\n",
      "weav\n",
      "web\n",
      "weed\n",
      "week\n",
      "weigh\n",
      "went\n",
      "west\n",
      "wet\n",
      "white\n",
      "wife\n",
      "wight\n",
      "wild\n",
      "willingli\n",
      "win\n",
      "winc\n",
      "wind\n",
      "window\n",
      "wire\n",
      "wish\n",
      "wit\n",
      "woman\n",
      "won\n",
      "wonder\n",
      "word\n",
      "work\n",
      "worn\n",
      "wound\n",
      "woven\n",
      "writer\n",
      "writh\n",
      "write\n",
      "written\n",
      "wrong\n",
      "wrote\n",
      "year\n",
      "ye\n",
      "yesterday\n",
      "young\n",
      "youngster\n",
      "zero\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Import packages\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Open file\n",
    "file = open('orange.txt', 'r')\n",
    " \n",
    "# read the file\n",
    "paragraph = file.read()\n",
    "\n",
    "# remove useless punctuation or marks\n",
    "sentences = paragraph.replace(\":\",'')\n",
    "\n",
    "# split the sentence to array\n",
    "splitted_sentences = sentences.split('.')\n",
    "\n",
    "# declare vectorizer with stop words in english\n",
    "vectorizer = TfidfVectorizer(stop_words ='english')\n",
    "\n",
    "# do feature extraxtion\n",
    "data = vectorizer.fit_transform(splitted_sentences)\n",
    "\n",
    "arr = numpy.asarray(vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stemmed_words = []\n",
    "stemmed_data = []\n",
    "for i in range(0, len(arr)):\n",
    "    # print(i)\n",
    "    if(i > 0):\n",
    "        # print('more than zero')\n",
    "        if(stemmed_words[len(stemmed_words) - 1] != ps.stem(arr[i])):\n",
    "            stemmed_words.append(ps.stem(arr[i]))\n",
    "        else:\n",
    "            previous_data = stemmed_words[len(stemmed_words) - 1]\n",
    "            numpy.delete(data.toarray(), i)\n",
    "    else:\n",
    "        # print('less than zero')\n",
    "        stemmed_words.append(ps.stem(arr[i]))       \n",
    "    \n",
    "\n",
    "\n",
    "# do feature extraxtion\n",
    "data = vectorizer.fit_transform(splitted_sentences, stemmed_words)\n",
    "for i in range(0, len(stemmed_words)):\n",
    "    print(stemmed_words[i])\n",
    "\n",
    "# data.shape\n",
    "# print(len(stemmed_words))\n",
    "# get stemmed words\n",
    "# arr = numpy.asarray(stemmed_words)\n",
    "\n",
    "# # save the result to file named output.csv\n",
    "# pd.DataFrame(data.todense()).to_csv('output.csv', sep=',', header=arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd78fef2128015050713e82ca51c6520b11aee7c9ee8df750520bbbc7384cbaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
