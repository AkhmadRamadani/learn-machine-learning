{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create manual normalization feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_feature(data):\n",
    "    n_max = max(data)\n",
    "    n_min = min(data)\n",
    "    len_of_data = len(data)\n",
    "    for i in range(len_of_data):\n",
    "        data[i] = (data[i] - n_min) / (n_max - n_min)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.375, 0.125, 0.5, 1.0]\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    10, 25, 15, 30, 50\n",
    "]\n",
    "\n",
    "print(norm_feature(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalisasi with SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100, 0.001], [8, 0.05], [50, 0.005], [88, 0.07], [4, 0.1]]\n",
      "[[1.0e+02 1.0e-03]\n",
      " [8.0e+00 5.0e-02]\n",
      " [5.0e+01 5.0e-03]\n",
      " [8.8e+01 7.0e-02]\n",
      " [4.0e+00 1.0e-01]]\n",
      "[[1.         0.        ]\n",
      " [0.04166667 0.49494949]\n",
      " [0.47916667 0.04040404]\n",
      " [0.875      0.6969697 ]\n",
      " [0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = [ \n",
    "    [100, 0.001],\n",
    "    [8, 0.05],\n",
    "    [50, 0.005],\n",
    "    [88, 0.07],\n",
    "    [4, 0.1],\n",
    "]\n",
    "\n",
    "print(data)\n",
    "\n",
    "print(numpy.asarray(data))\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaled = scaler.fit_transform(numpy.asarray(data))\n",
    "print(scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean, pstdev\n",
    "\n",
    "\n",
    "def std_feature(data):\n",
    "    kolom_data = data.shape[1]\n",
    "    baris_data = data.shape[0]\n",
    "\n",
    "    for i in range(0, baris_data):\n",
    "        for j in range(0,kolom_data):\n",
    "            mean_data = mean(data[:,j])\n",
    "            std_data = pstdev(data[:,j])\n",
    "            data[i][j] = (data[i][j] - mean_data) / std_data \n",
    "            \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.26398112 -1.16389967]\n",
      " [-0.65623116  0.48622176]\n",
      " [ 0.6102111   0.18921441]\n",
      " [ 1.99807462  0.23476602]\n",
      " [ 1.65186847  0.22506085]]\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    [100, 0.001],\n",
    "    [8, 0.05],\n",
    "    [50, 0.005],\n",
    "    [88, 0.07],\n",
    "    [4, 0.1],\n",
    "]\n",
    "\n",
    "data = numpy.asarray(data)\n",
    "\n",
    "print(std_feature(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardisasi with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100, 0.001], [8, 0.05], [50, 0.005], [88, 0.07], [4, 0.1]]\n",
      "[[ 1.26398112 -1.16389967]\n",
      " [-1.06174414  0.12639634]\n",
      " [ 0.         -1.05856939]\n",
      " [ 0.96062565  0.65304778]\n",
      " [-1.16286263  1.44302493]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import asarray\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = [\n",
    "    [100, 0.001],\n",
    "    [8, 0.05],\n",
    "    [50, 0.005],\n",
    "    [88, 0.07],\n",
    "    [4, 0.1],\n",
    "]\n",
    "\n",
    "print(data)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaled = scaler.fit_transform(data)\n",
    "print(scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction dari Data Kategorik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "data = [\n",
    "    ['POLINEMA'],\n",
    "    ['PENS'], \n",
    "    ['PNJ'], \n",
    "    ['PNP'],\n",
    "    ['POLBAN']\n",
    "]\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "encoded = encoder.fit_transform(data)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "data = [\n",
    "    ['POLINEMA'],\n",
    "    ['PENS'],\n",
    "    ['PNJ'],\n",
    "    ['PNP'],\n",
    "    ['POLBAN']\n",
    "]\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoded = encoder.fit_transform(data)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction pada Data Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ate' 'away' 'cat' 'end' 'finally' 'house' 'little' 'mouse' 'ran' 'saw'\n",
      " 'story' 'tiny']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.4755751 , 0.58946308, 0.28088232, 0.        , 0.        ,\n",
       "         0.        , 0.58946308],\n",
       "        [0.        , 0.        , 0.58873218, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.34771471, 0.        , 0.72971837,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.58946308, 0.        , 0.        , 0.        ,\n",
       "         0.4755751 , 0.        , 0.28088232, 0.58946308, 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.58946308, 0.        , 0.4755751 , 0.        , 0.58946308,\n",
       "         0.        , 0.        , 0.28088232, 0.        , 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.67009179, 0.        ,\n",
       "         0.        , 0.        , 0.31930233, 0.        , 0.        ,\n",
       "         0.67009179, 0.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'the house had a tiny little mouse',\n",
    "    'the cat saw the mouse', \n",
    "    'the mouse ran away from the house',\n",
    "    'the cat finally ate the mouse', \n",
    "    'the end of the mouse story'\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words ='english')\n",
    "data = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "# print(data.todense())\n",
    "data.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Import packages\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy\n",
    "import pandas as pd\n",
    "\n",
    "# Open file\n",
    "file = open('orange.txt', 'r')\n",
    " \n",
    "# read the file\n",
    "paragraph = file.read()\n",
    "\n",
    "# remove useless punctuation or marks\n",
    "sentences = paragraph.replace(\":\",'')\n",
    "\n",
    "# split the sentence to array\n",
    "splitted_sentences = sentences.split('.')\n",
    "\n",
    "# declare vectorizer with stop words in english\n",
    "vectorizer = TfidfVectorizer(stop_words ='english')\n",
    "\n",
    "# do feature extraxtion\n",
    "data = vectorizer.fit_transform(splitted_sentences)\n",
    "\n",
    "# get stemmed words\n",
    "arr = numpy.asarray(vectorizer.get_feature_names_out())\n",
    "\n",
    "# save the result to file named output.csv\n",
    "pd.DataFrame(data.todense()).to_csv('output.csv', sep=',', header=arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd78fef2128015050713e82ca51c6520b11aee7c9ee8df750520bbbc7384cbaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
